{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Preparing Colab"
      ],
      "metadata": {
        "id": "xS0lzxsaRSvj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "def getLocalFiles():\n",
        "    _files = files.upload()\n",
        "    if len(_files) >0:\n",
        "       for k,v in _files.items():\n",
        "         open(k,'wb').write(v)\n",
        "getLocalFiles()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "id": "5qQkwQ2DUYjw",
        "outputId": "7b512c0c-0f02-40f7-f3ac-32e8d5212f2d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-80463b0d-93e8-434a-99f9-8f98736ddcf4\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-80463b0d-93e8-434a-99f9-8f98736ddcf4\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving configure.py to configure.py\n",
            "Saving data.zip to data.zip\n",
            "Saving data_collector.py to data_collector.py\n",
            "Saving model.py to model.py\n",
            "Saving predict_from_chkpnt.py to predict_from_chkpnt.py\n",
            "Saving run_model.py to run_model.py\n",
            "Saving saved_model.zip to saved_model.zip\n",
            "Saving utility.py to utility.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from zipfile import ZipFile\n",
        "with ZipFile('data.zip', 'r') as zip:\n",
        "  zip.extractall()\n",
        "  print('Done data')\n",
        "with ZipFile('saved_model.zip', 'r') as zip:\n",
        "  zip.extractall()\n",
        "  print('Done saved_model')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7gDvWR7CYqlI",
        "outputId": "f6bd8ef4-e067-4dc2-9e2a-533d03159bea"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done data\n",
            "Done saved_model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQml8QT6REtR"
      },
      "source": [
        "# RAPTA Demo Version\n",
        "This demo version is for tutorial purpose. Here we analyzed only one file of PT_S38417 for 0.78V."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qwU8l557REtU"
      },
      "outputs": [],
      "source": [
        "# Python Libraries\n",
        "import time\n",
        "import pickle\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69uPvtQEREtV"
      },
      "source": [
        "# Repositories\n",
        "All the necessary codes have been developed in different files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SHbO0UAIREtV",
        "outputId": "cc46d198-442c-4e73-a0df-740d90340c70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on the CPU\n",
            "Running on the CPU\n"
          ]
        }
      ],
      "source": [
        "# Customized Libraries\n",
        "import configure as config # For model configuration\n",
        "import data_collector # For data processing\n",
        "import model # Model has been developed here\n",
        "import run_model # Model training related code\n",
        "import predict_from_chkpnt # Testing related code\n",
        "import utility # Any other necessary functions for this project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDR_KtKIREtW"
      },
      "source": [
        "## Data Generation and Preparation:\n",
        "We used Synopsys IC Compiler (ICC) for the physical design of benchmarks in 32nm technology available under Synopsys educational license. We used the ICC timing report to extract a set of timing paths and queried ICC to extract the gate and net properties. We used one-hot encoding to represent the value of non-numerical gate properties, such as \"gate type‚Äù or \"threshold voltage‚Äù. Synopsys Primetime was then launched for path-based timing analysis, and the delay of each subpath (ùëÉùê∑ , ùëÉùê∂, and ùëÉùêø) was collected as labels and sub-labels for each timing path, respectively.\n",
        "\n",
        "Raw data is in json format. We formatted and converted the data in pickle format. The necessary functions are in the \"data_collector.py\" file. So, we import that here at first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZtBQXOsREtX",
        "outputId": "65e42f34-393f-490a-a56a-a8c054562728"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " starting file processing v_0.78\n",
            "file_processed v_0.78\n",
            "preprocess_data v_0.78, file processing time 0.4244836190000001, preprocessing_data 3.5743583660000127\n",
            " starting file processing v_0.78\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/data_collector.py:37: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  X_arr = np.array(X)\n",
            "/content/data_collector.py:132: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  X_new = np.array(X_new) # l, d, c, VS, sublabel, label\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "preprocess_data v_0.78\n"
          ]
        }
      ],
      "source": [
        "fp=config.fp[0:-1]\n",
        "raw_input_file = config.processed_saved_path + fp + '/jason.json'\n",
        "processed_saved_path = config.processed_saved_path + fp + '/'\n",
        "processed_file = config.processed_saved_path + fp + '/processed.json'\n",
        "print(' starting file processing {}'.format(fp))\n",
        "file_processing_tic = time.perf_counter()\n",
        "data_collector.file_processing(raw_input_file, processed_file)\n",
        "file_processing_toc = time.perf_counter()\n",
        "print('file_processed {}'.format(fp))\n",
        "data_collector.preprocess_data(processed_file, processed_saved_path, fp)\n",
        "preprocess_data_toc = time.perf_counter()\n",
        "print('preprocess_data {}, file processing time {}, preprocessing_data {}'.format(fp, (\n",
        "            file_processing_toc - file_processing_tic), (preprocess_data_toc - file_processing_tic)))\n",
        "X = pickle.load(open(config.exp_data_path + fp + '_X_gate.pk', 'rb'))\n",
        "X_sublabel = pickle.load(open(config.exp_data_path + fp + '_X_sublabels.pk', 'rb'))\n",
        "Y = pickle.load(open(config.exp_data_path + fp + '_Y_gate.pk', 'rb'))\n",
        "saving_Path_Train = config.exp_data_path + fp + '_train.pk'\n",
        "saving_Path_Test = config.exp_data_path + fp + '_test.pk'\n",
        "print(' starting file processing {}'.format(fp))\n",
        "data_collector.createFromProcessed(X, X_sublabel, Y, saving_Path_Train, saving_Path_Test)\n",
        "print('preprocess_data {}'.format(fp))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRZz1wTyREtY"
      },
      "source": [
        "## Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-o1vUYY9REtY",
        "outputId": "8cd3d897-5dc3-4f10-9995-cd016493cc1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data/model_purpose/PT_S38417/v_0.78_train.pk\n",
            "3324\n",
            "Running Main for v_0.78_\n",
            "Running tensormaker\n",
            "Train Input Data Fit Transformed\n",
            "Train Target Data Fit Transformed\n",
            "Running tensormaker\n",
            "Test Input Data Transformed\n",
            "Test Target Data Transformed\n",
            "Iteration: 0/10  Current Loss: 0.6858076453208923 and previously recorded Min Loss None\n",
            "Iteration: 0/20  Current Loss: 0.26422178745269775 and previously recorded Min Loss 0.6858076453208923\n",
            " Model saved\n",
            "Iteration: 1/30  Current Loss: 0.08295884728431702 and previously recorded Min Loss 0.26422178745269775\n",
            " Model saved\n",
            "Iteration: 1/40  Current Loss: 0.055782463401556015 and previously recorded Min Loss 0.08295884728431702\n",
            " Model saved\n",
            "Iteration: 1/50  Current Loss: 0.0260595865547657 and previously recorded Min Loss 0.055782463401556015\n",
            " Model saved\n",
            "Iteration: 2/60  Current Loss: 0.02114274352788925 and previously recorded Min Loss 0.0260595865547657\n",
            " Model saved\n",
            "Iteration: 2/70  Current Loss: 0.01573525369167328 and previously recorded Min Loss 0.02114274352788925\n",
            " Model saved\n",
            "Iteration: 3/80  Current Loss: 0.010821484960615635 and previously recorded Min Loss 0.01573525369167328\n",
            " Model saved\n",
            "Iteration: 3/90  Current Loss: 0.008353191427886486 and previously recorded Min Loss 0.010821484960615635\n",
            " Model saved\n",
            "Iteration: 3/100  Current Loss: 0.006820694077759981 and previously recorded Min Loss 0.008353191427886486\n",
            " Model saved\n",
            "Iteration: 4/110  Current Loss: 0.006120563019067049 and previously recorded Min Loss 0.006820694077759981\n",
            " Model saved\n",
            "Iteration: 4/120  Current Loss: 0.006062227301299572 and previously recorded Min Loss 0.006120563019067049\n",
            " Model saved\n",
            "Iteration: 5/130  Current Loss: 0.005090777762234211 and previously recorded Min Loss 0.006062227301299572\n",
            " Model saved\n",
            "Iteration: 5/140  Current Loss: 0.004927625879645348 and previously recorded Min Loss 0.005090777762234211\n",
            " Model saved\n",
            "Iteration: 5/150  Current Loss: 0.005229944828897715 and previously recorded Min Loss 0.004927625879645348\n",
            "Iteration: 6/160  Current Loss: 0.004453853704035282 and previously recorded Min Loss 0.004927625879645348\n",
            " Model saved\n",
            "Iteration: 6/170  Current Loss: 0.004085191059857607 and previously recorded Min Loss 0.004453853704035282\n",
            " Model saved\n",
            "Iteration: 7/180  Current Loss: 0.005060854833573103 and previously recorded Min Loss 0.004085191059857607\n",
            "Iteration: 7/190  Current Loss: 0.004511268809437752 and previously recorded Min Loss 0.004085191059857607\n",
            "Iteration: 7/200  Current Loss: 0.004549102392047644 and previously recorded Min Loss 0.004085191059857607\n",
            "Iteration: 8/210  Current Loss: 0.0035777699667960405 and previously recorded Min Loss 0.004085191059857607\n",
            " Model saved\n",
            "Iteration: 8/220  Current Loss: 0.0036187039222568274 and previously recorded Min Loss 0.0035777699667960405\n",
            "Iteration: 9/230  Current Loss: 0.003311226377263665 and previously recorded Min Loss 0.0035777699667960405\n",
            " Model saved\n",
            "Iteration: 9/240  Current Loss: 0.0028342464938759804 and previously recorded Min Loss 0.003311226377263665\n",
            " Model saved\n",
            "Iteration: 9/250  Current Loss: 0.002998266601935029 and previously recorded Min Loss 0.0028342464938759804\n",
            "Iteration: 10/260  Current Loss: 0.0032186300959438086 and previously recorded Min Loss 0.0028342464938759804\n",
            "Iteration: 10/270  Current Loss: 0.0026257375720888376 and previously recorded Min Loss 0.0028342464938759804\n",
            " Model saved\n",
            "Iteration: 11/280  Current Loss: 0.0027348301373422146 and previously recorded Min Loss 0.0026257375720888376\n",
            "Iteration: 11/290  Current Loss: 0.002493208507075906 and previously recorded Min Loss 0.0026257375720888376\n",
            " Model saved\n",
            "Iteration: 11/300  Current Loss: 0.0024824996944516897 and previously recorded Min Loss 0.002493208507075906\n",
            " Model saved\n",
            "Iteration: 12/310  Current Loss: 0.002236909233033657 and previously recorded Min Loss 0.0024824996944516897\n",
            " Model saved\n",
            "Iteration: 12/320  Current Loss: 0.002165775978937745 and previously recorded Min Loss 0.002236909233033657\n",
            " Model saved\n",
            "Iteration: 13/330  Current Loss: 0.0023757268209010363 and previously recorded Min Loss 0.002165775978937745\n",
            "Iteration: 13/340  Current Loss: 0.0021826252341270447 and previously recorded Min Loss 0.002165775978937745\n",
            "Iteration: 13/350  Current Loss: 0.003009332809597254 and previously recorded Min Loss 0.002165775978937745\n",
            "Iteration: 14/360  Current Loss: 0.0020265760831534863 and previously recorded Min Loss 0.002165775978937745\n",
            " Model saved\n",
            "Iteration: 14/370  Current Loss: 0.002408105880022049 and previously recorded Min Loss 0.0020265760831534863\n",
            "Iteration: 15/380  Current Loss: 0.001977088861167431 and previously recorded Min Loss 0.0020265760831534863\n",
            " Model saved\n",
            "Iteration: 15/390  Current Loss: 0.001969274366274476 and previously recorded Min Loss 0.001977088861167431\n",
            " Model saved\n",
            "Iteration: 15/400  Current Loss: 0.0018001709831878543 and previously recorded Min Loss 0.001969274366274476\n",
            " Model saved\n",
            "Iteration: 16/410  Current Loss: 0.0018608227837830782 and previously recorded Min Loss 0.0018001709831878543\n",
            "Iteration: 16/420  Current Loss: 0.0030156744178384542 and previously recorded Min Loss 0.0018001709831878543\n",
            "Iteration: 17/430  Current Loss: 0.001940412912517786 and previously recorded Min Loss 0.0018001709831878543\n",
            "Iteration: 17/440  Current Loss: 0.001867083366960287 and previously recorded Min Loss 0.0018001709831878543\n",
            "Iteration: 17/450  Current Loss: 0.001987246796488762 and previously recorded Min Loss 0.0018001709831878543\n",
            "Iteration: 18/460  Current Loss: 0.00330981332808733 and previously recorded Min Loss 0.0018001709831878543\n",
            "Iteration: 18/470  Current Loss: 0.0018126820214092731 and previously recorded Min Loss 0.0018001709831878543\n",
            "Iteration: 19/480  Current Loss: 0.0016243556747213006 and previously recorded Min Loss 0.0018001709831878543\n",
            " Model saved\n",
            "Iteration: 19/490  Current Loss: 0.0015755074564367533 and previously recorded Min Loss 0.0016243556747213006\n",
            " Model saved\n",
            "Iteration: 19/500  Current Loss: 0.0015558867016807199 and previously recorded Min Loss 0.0015755074564367533\n",
            " Model saved\n",
            "Iteration: 20/510  Current Loss: 0.0021525509655475616 and previously recorded Min Loss 0.0015558867016807199\n",
            "Iteration: 20/520  Current Loss: 0.0022003783378750086 and previously recorded Min Loss 0.0015558867016807199\n",
            "Iteration: 21/530  Current Loss: 0.0014622354647144675 and previously recorded Min Loss 0.0015558867016807199\n",
            " Model saved\n",
            "Iteration: 21/540  Current Loss: 0.0015634802402928472 and previously recorded Min Loss 0.0014622354647144675\n",
            "Iteration: 21/550  Current Loss: 0.0014705330831930041 and previously recorded Min Loss 0.0014622354647144675\n",
            "Iteration: 22/560  Current Loss: 0.001450027571991086 and previously recorded Min Loss 0.0014622354647144675\n",
            " Model saved\n",
            "Iteration: 22/570  Current Loss: 0.001508402288891375 and previously recorded Min Loss 0.001450027571991086\n",
            "Iteration: 23/580  Current Loss: 0.001731189200654626 and previously recorded Min Loss 0.001450027571991086\n",
            "Iteration: 23/590  Current Loss: 0.001570325461216271 and previously recorded Min Loss 0.001450027571991086\n",
            "Iteration: 23/600  Current Loss: 0.0014463098486885428 and previously recorded Min Loss 0.001450027571991086\n",
            " Model saved\n",
            "Iteration: 24/610  Current Loss: 0.001447150600142777 and previously recorded Min Loss 0.0014463098486885428\n",
            "Iteration: 24/620  Current Loss: 0.0014584778109565377 and previously recorded Min Loss 0.0014463098486885428\n",
            "Iteration: 25/630  Current Loss: 0.0013222481356933713 and previously recorded Min Loss 0.0014463098486885428\n",
            " Model saved\n",
            "Iteration: 25/640  Current Loss: 0.001387080759741366 and previously recorded Min Loss 0.0013222481356933713\n",
            "Iteration: 25/650  Current Loss: 0.0015702322125434875 and previously recorded Min Loss 0.0013222481356933713\n",
            "Iteration: 26/660  Current Loss: 0.0014911217149347067 and previously recorded Min Loss 0.0013222481356933713\n",
            "Iteration: 26/670  Current Loss: 0.0013424495700746775 and previously recorded Min Loss 0.0013222481356933713\n",
            "Iteration: 27/680  Current Loss: 0.0012851400533691049 and previously recorded Min Loss 0.0013222481356933713\n",
            " Model saved\n",
            "Iteration: 27/690  Current Loss: 0.0014908862067386508 and previously recorded Min Loss 0.0012851400533691049\n",
            "Iteration: 27/700  Current Loss: 0.0012650641147047281 and previously recorded Min Loss 0.0012851400533691049\n",
            " Model saved\n",
            "Iteration: 28/710  Current Loss: 0.0013557904167100787 and previously recorded Min Loss 0.0012650641147047281\n",
            "Iteration: 28/720  Current Loss: 0.0012614508159458637 and previously recorded Min Loss 0.0012650641147047281\n",
            " Model saved\n",
            "Iteration: 29/730  Current Loss: 0.0016084028175100684 and previously recorded Min Loss 0.0012614508159458637\n",
            "Iteration: 29/740  Current Loss: 0.0012476545525714755 and previously recorded Min Loss 0.0012614508159458637\n",
            " Model saved\n",
            "Iteration: 29/750  Current Loss: 0.0016725119203329086 and previously recorded Min Loss 0.0012476545525714755\n",
            "Iteration: 30/760  Current Loss: 0.001659667817875743 and previously recorded Min Loss 0.0012476545525714755\n",
            "Iteration: 30/770  Current Loss: 0.0014668537769466639 and previously recorded Min Loss 0.0012476545525714755\n",
            "Iteration: 31/780  Current Loss: 0.0011704209027811885 and previously recorded Min Loss 0.0012476545525714755\n",
            " Model saved\n",
            "Iteration: 31/790  Current Loss: 0.0012264796532690525 and previously recorded Min Loss 0.0011704209027811885\n",
            "Iteration: 31/800  Current Loss: 0.0012247973354533315 and previously recorded Min Loss 0.0011704209027811885\n",
            "Iteration: 32/810  Current Loss: 0.0011814865283668041 and previously recorded Min Loss 0.0011704209027811885\n",
            "Iteration: 32/820  Current Loss: 0.001794591429643333 and previously recorded Min Loss 0.0011704209027811885\n",
            "Iteration: 33/830  Current Loss: 0.0011988073820248246 and previously recorded Min Loss 0.0011704209027811885\n",
            "Iteration: 33/840  Current Loss: 0.0012501113815233111 and previously recorded Min Loss 0.0011704209027811885\n",
            "Iteration: 33/850  Current Loss: 0.0012022454757243395 and previously recorded Min Loss 0.0011704209027811885\n",
            "Iteration: 34/860  Current Loss: 0.001183843705803156 and previously recorded Min Loss 0.0011704209027811885\n",
            "Iteration: 34/870  Current Loss: 0.0013022244675084949 and previously recorded Min Loss 0.0011704209027811885\n",
            "Iteration: 35/880  Current Loss: 0.0011235135607421398 and previously recorded Min Loss 0.0011704209027811885\n",
            " Model saved\n",
            "Iteration: 35/890  Current Loss: 0.00115393556188792 and previously recorded Min Loss 0.0011235135607421398\n",
            "Iteration: 35/900  Current Loss: 0.0012659684289246798 and previously recorded Min Loss 0.0011235135607421398\n",
            "Iteration: 36/910  Current Loss: 0.0012203154619783163 and previously recorded Min Loss 0.0011235135607421398\n",
            "Iteration: 36/920  Current Loss: 0.0010967464186251163 and previously recorded Min Loss 0.0011235135607421398\n",
            " Model saved\n",
            "Iteration: 37/930  Current Loss: 0.001129611860960722 and previously recorded Min Loss 0.0010967464186251163\n",
            "Iteration: 37/940  Current Loss: 0.0011641440214589238 and previously recorded Min Loss 0.0010967464186251163\n",
            "Iteration: 37/950  Current Loss: 0.0011121787829324603 and previously recorded Min Loss 0.0010967464186251163\n",
            "Iteration: 38/960  Current Loss: 0.0011299134930595756 and previously recorded Min Loss 0.0010967464186251163\n",
            "Iteration: 38/970  Current Loss: 0.0011131049832329154 and previously recorded Min Loss 0.0010967464186251163\n",
            "Iteration: 39/980  Current Loss: 0.0010792914545163512 and previously recorded Min Loss 0.0010967464186251163\n",
            " Model saved\n",
            "Iteration: 39/990  Current Loss: 0.0010267870966345072 and previously recorded Min Loss 0.0010792914545163512\n",
            " Model saved\n",
            "Iteration: 39/1000  Current Loss: 0.001077838707715273 and previously recorded Min Loss 0.0010267870966345072\n",
            "Iteration: 40/1010  Current Loss: 0.001094826147891581 and previously recorded Min Loss 0.0010267870966345072\n",
            "Iteration: 40/1020  Current Loss: 0.0009952319087460637 and previously recorded Min Loss 0.0010267870966345072\n",
            " Model saved\n",
            "Iteration: 41/1030  Current Loss: 0.0011444016126915812 and previously recorded Min Loss 0.0009952319087460637\n",
            "Iteration: 41/1040  Current Loss: 0.0012453945819288492 and previously recorded Min Loss 0.0009952319087460637\n",
            "Iteration: 41/1050  Current Loss: 0.000998159172013402 and previously recorded Min Loss 0.0009952319087460637\n",
            "Iteration: 42/1060  Current Loss: 0.0009902551537379622 and previously recorded Min Loss 0.0009952319087460637\n",
            " Model saved\n",
            "Iteration: 42/1070  Current Loss: 0.001142471912316978 and previously recorded Min Loss 0.0009902551537379622\n",
            "Iteration: 43/1080  Current Loss: 0.0009519293671473861 and previously recorded Min Loss 0.0009902551537379622\n",
            " Model saved\n",
            "Iteration: 43/1090  Current Loss: 0.00097662303596735 and previously recorded Min Loss 0.0009519293671473861\n",
            "Iteration: 43/1100  Current Loss: 0.0010211350163444877 and previously recorded Min Loss 0.0009519293671473861\n",
            "Iteration: 44/1110  Current Loss: 0.0009988858364522457 and previously recorded Min Loss 0.0009519293671473861\n",
            "Iteration: 44/1120  Current Loss: 0.001018981565721333 and previously recorded Min Loss 0.0009519293671473861\n",
            "Iteration: 45/1130  Current Loss: 0.000990547239780426 and previously recorded Min Loss 0.0009519293671473861\n",
            "Iteration: 45/1140  Current Loss: 0.0009396265377290547 and previously recorded Min Loss 0.0009519293671473861\n",
            " Model saved\n",
            "Iteration: 45/1150  Current Loss: 0.0011304348008707166 and previously recorded Min Loss 0.0009396265377290547\n",
            "Iteration: 46/1160  Current Loss: 0.0010912332218140364 and previously recorded Min Loss 0.0009396265377290547\n",
            "Iteration: 46/1170  Current Loss: 0.0015373514033854008 and previously recorded Min Loss 0.0009396265377290547\n",
            "Iteration: 47/1180  Current Loss: 0.0011031213216483593 and previously recorded Min Loss 0.0009396265377290547\n",
            "Iteration: 47/1190  Current Loss: 0.001072225975804031 and previously recorded Min Loss 0.0009396265377290547\n",
            "Iteration: 47/1200  Current Loss: 0.0009342521661892533 and previously recorded Min Loss 0.0009396265377290547\n",
            " Model saved\n",
            "Iteration: 48/1210  Current Loss: 0.0009721244568936527 and previously recorded Min Loss 0.0009342521661892533\n",
            "Iteration: 48/1220  Current Loss: 0.0009677879861555994 and previously recorded Min Loss 0.0009342521661892533\n",
            "Iteration: 49/1230  Current Loss: 0.0009832988725975156 and previously recorded Min Loss 0.0009342521661892533\n",
            "Iteration: 49/1240  Current Loss: 0.0009906049817800522 and previously recorded Min Loss 0.0009342521661892533\n",
            "Iteration: 49/1250  Current Loss: 0.0011157883564010262 and previously recorded Min Loss 0.0009342521661892533\n",
            "Iteration: 50/1260  Current Loss: 0.0010523790260776877 and previously recorded Min Loss 0.0009342521661892533\n",
            "Iteration: 50/1270  Current Loss: 0.001454391865991056 and previously recorded Min Loss 0.0009342521661892533\n",
            "Iteration: 51/1280  Current Loss: 0.0013193375198170543 and previously recorded Min Loss 0.0009342521661892533\n",
            "Iteration: 51/1290  Current Loss: 0.0010431816335767508 and previously recorded Min Loss 0.0009342521661892533\n",
            "Iteration: 51/1300  Current Loss: 0.0009945586789399385 and previously recorded Min Loss 0.0009342521661892533\n",
            "Iteration: 52/1310  Current Loss: 0.000993420835584402 and previously recorded Min Loss 0.0009342521661892533\n",
            "Iteration: 52/1320  Current Loss: 0.0009052252862602472 and previously recorded Min Loss 0.0009342521661892533\n",
            " Model saved\n",
            "Iteration: 53/1330  Current Loss: 0.0010192911140620708 and previously recorded Min Loss 0.0009052252862602472\n",
            "Iteration: 53/1340  Current Loss: 0.0009152152924798429 and previously recorded Min Loss 0.0009052252862602472\n",
            "Iteration: 53/1350  Current Loss: 0.0014279716415330768 and previously recorded Min Loss 0.0009052252862602472\n",
            "Iteration: 54/1360  Current Loss: 0.0009399205446243286 and previously recorded Min Loss 0.0009052252862602472\n",
            "Iteration: 54/1370  Current Loss: 0.0009064598707482219 and previously recorded Min Loss 0.0009052252862602472\n",
            "Iteration: 55/1380  Current Loss: 0.0010720340069383383 and previously recorded Min Loss 0.0009052252862602472\n",
            "Iteration: 55/1390  Current Loss: 0.0010410347022116184 and previously recorded Min Loss 0.0009052252862602472\n",
            "Iteration: 55/1400  Current Loss: 0.0011513627832755446 and previously recorded Min Loss 0.0009052252862602472\n",
            "Iteration: 56/1410  Current Loss: 0.0008715930744074285 and previously recorded Min Loss 0.0009052252862602472\n",
            " Model saved\n",
            "Iteration: 56/1420  Current Loss: 0.0009211087599396706 and previously recorded Min Loss 0.0008715930744074285\n",
            "Iteration: 57/1430  Current Loss: 0.0010325039038434625 and previously recorded Min Loss 0.0008715930744074285\n",
            "Iteration: 57/1440  Current Loss: 0.0010056786704808474 and previously recorded Min Loss 0.0008715930744074285\n",
            "Iteration: 57/1450  Current Loss: 0.0010548944119364023 and previously recorded Min Loss 0.0008715930744074285\n",
            "Iteration: 58/1460  Current Loss: 0.0010462913196533918 and previously recorded Min Loss 0.0008715930744074285\n",
            "Iteration: 58/1470  Current Loss: 0.0009422498405911028 and previously recorded Min Loss 0.0008715930744074285\n",
            "Iteration: 59/1480  Current Loss: 0.0017647010972723365 and previously recorded Min Loss 0.0008715930744074285\n",
            "Iteration: 59/1490  Current Loss: 0.0014976633246988058 and previously recorded Min Loss 0.0008715930744074285\n",
            "Iteration: 59/1500  Current Loss: 0.001413285150192678 and previously recorded Min Loss 0.0008715930744074285\n",
            "Iteration: 60/1510  Current Loss: 0.0010097341146320105 and previously recorded Min Loss 0.0008715930744074285\n",
            "Iteration: 60/1520  Current Loss: 0.000929580710362643 and previously recorded Min Loss 0.0008715930744074285\n",
            "Iteration: 61/1530  Current Loss: 0.0008398824138566852 and previously recorded Min Loss 0.0008715930744074285\n",
            " Model saved\n",
            "Iteration: 61/1540  Current Loss: 0.0009031087392941117 and previously recorded Min Loss 0.0008398824138566852\n",
            "Iteration: 61/1550  Current Loss: 0.0008582239388488233 and previously recorded Min Loss 0.0008398824138566852\n",
            "Iteration: 62/1560  Current Loss: 0.00082629662938416 and previously recorded Min Loss 0.0008398824138566852\n",
            " Model saved\n",
            "Iteration: 62/1570  Current Loss: 0.0008342054206877947 and previously recorded Min Loss 0.00082629662938416\n",
            "Iteration: 63/1580  Current Loss: 0.0009028934291563928 and previously recorded Min Loss 0.00082629662938416\n",
            "Iteration: 63/1590  Current Loss: 0.0012522878823801875 and previously recorded Min Loss 0.00082629662938416\n",
            "Iteration: 63/1600  Current Loss: 0.0010323537280783057 and previously recorded Min Loss 0.00082629662938416\n",
            "Iteration: 64/1610  Current Loss: 0.0009934426052495837 and previously recorded Min Loss 0.00082629662938416\n",
            "Iteration: 64/1620  Current Loss: 0.0008517364622093737 and previously recorded Min Loss 0.00082629662938416\n",
            "Iteration: 65/1630  Current Loss: 0.001312422682531178 and previously recorded Min Loss 0.00082629662938416\n",
            "Iteration: 65/1640  Current Loss: 0.0008638832368887961 and previously recorded Min Loss 0.00082629662938416\n",
            "Iteration: 65/1650  Current Loss: 0.0008957774261943996 and previously recorded Min Loss 0.00082629662938416\n",
            "Iteration: 66/1660  Current Loss: 0.0009252080344595015 and previously recorded Min Loss 0.00082629662938416\n",
            "Iteration: 66/1670  Current Loss: 0.0009088771184906363 and previously recorded Min Loss 0.00082629662938416\n",
            "Iteration: 67/1680  Current Loss: 0.0008183292229659855 and previously recorded Min Loss 0.00082629662938416\n",
            " Model saved\n",
            "Iteration: 67/1690  Current Loss: 0.0008152515511028469 and previously recorded Min Loss 0.0008183292229659855\n",
            " Model saved\n",
            "Iteration: 67/1700  Current Loss: 0.0009751636534929276 and previously recorded Min Loss 0.0008152515511028469\n",
            "Iteration: 68/1710  Current Loss: 0.0008427483262494206 and previously recorded Min Loss 0.0008152515511028469\n",
            "Iteration: 68/1720  Current Loss: 0.0008257758454419672 and previously recorded Min Loss 0.0008152515511028469\n",
            "Iteration: 69/1730  Current Loss: 0.0008131366921588778 and previously recorded Min Loss 0.0008152515511028469\n",
            " Model saved\n",
            "Iteration: 69/1740  Current Loss: 0.0007970202714204788 and previously recorded Min Loss 0.0008131366921588778\n",
            " Model saved\n",
            "Iteration: 69/1750  Current Loss: 0.0007611144683323801 and previously recorded Min Loss 0.0007970202714204788\n",
            " Model saved\n",
            "Iteration: 70/1760  Current Loss: 0.0007638027309440076 and previously recorded Min Loss 0.0007611144683323801\n",
            "Iteration: 70/1770  Current Loss: 0.000762318552006036 and previously recorded Min Loss 0.0007611144683323801\n",
            "Iteration: 71/1780  Current Loss: 0.0007986618438735604 and previously recorded Min Loss 0.0007611144683323801\n",
            "Iteration: 71/1790  Current Loss: 0.0007997973589226604 and previously recorded Min Loss 0.0007611144683323801\n",
            "Iteration: 71/1800  Current Loss: 0.0008835410699248314 and previously recorded Min Loss 0.0007611144683323801\n",
            "Iteration: 72/1810  Current Loss: 0.0008383756503462791 and previously recorded Min Loss 0.0007611144683323801\n",
            "Iteration: 72/1820  Current Loss: 0.000784940377343446 and previously recorded Min Loss 0.0007611144683323801\n",
            "Iteration: 73/1830  Current Loss: 0.0009200983331538737 and previously recorded Min Loss 0.0007611144683323801\n",
            "Iteration: 73/1840  Current Loss: 0.0009357113158330321 and previously recorded Min Loss 0.0007611144683323801\n",
            "Iteration: 73/1850  Current Loss: 0.0007620352553203702 and previously recorded Min Loss 0.0007611144683323801\n",
            "Iteration: 74/1860  Current Loss: 0.000765374512411654 and previously recorded Min Loss 0.0007611144683323801\n",
            "Iteration: 74/1870  Current Loss: 0.0007454470614902675 and previously recorded Min Loss 0.0007611144683323801\n",
            " Model saved\n",
            "Iteration: 75/1880  Current Loss: 0.0007603959529660642 and previously recorded Min Loss 0.0007454470614902675\n",
            "Iteration: 75/1890  Current Loss: 0.0010139086516574025 and previously recorded Min Loss 0.0007454470614902675\n",
            "Iteration: 75/1900  Current Loss: 0.0008031956385821104 and previously recorded Min Loss 0.0007454470614902675\n",
            "Iteration: 76/1910  Current Loss: 0.0008477868977934122 and previously recorded Min Loss 0.0007454470614902675\n",
            "Iteration: 76/1920  Current Loss: 0.0008418016950599849 and previously recorded Min Loss 0.0007454470614902675\n",
            "Iteration: 77/1930  Current Loss: 0.0008089598850347102 and previously recorded Min Loss 0.0007454470614902675\n",
            "Iteration: 77/1940  Current Loss: 0.001243116450496018 and previously recorded Min Loss 0.0007454470614902675\n",
            "Iteration: 77/1950  Current Loss: 0.0009899786673486233 and previously recorded Min Loss 0.0007454470614902675\n",
            "Iteration: 78/1960  Current Loss: 0.0008509624749422073 and previously recorded Min Loss 0.0007454470614902675\n",
            "Iteration: 78/1970  Current Loss: 0.0007947413832880557 and previously recorded Min Loss 0.0007454470614902675\n",
            "Iteration: 79/1980  Current Loss: 0.0008325513917952776 and previously recorded Min Loss 0.0007454470614902675\n",
            "Iteration: 79/1990  Current Loss: 0.0008589944336563349 and previously recorded Min Loss 0.0007454470614902675\n",
            "Iteration: 79/2000  Current Loss: 0.0007859341567382216 and previously recorded Min Loss 0.0007454470614902675\n",
            "Iteration: 80/2010  Current Loss: 0.0007846917724236846 and previously recorded Min Loss 0.0007454470614902675\n",
            "Iteration: 80/2020  Current Loss: 0.0007784049957990646 and previously recorded Min Loss 0.0007454470614902675\n",
            "Iteration: 81/2030  Current Loss: 0.000777633860707283 and previously recorded Min Loss 0.0007454470614902675\n",
            "Iteration: 81/2040  Current Loss: 0.0008854226907715201 and previously recorded Min Loss 0.0007454470614902675\n",
            "Iteration: 81/2050  Current Loss: 0.000934499897994101 and previously recorded Min Loss 0.0007454470614902675\n",
            "Iteration: 82/2060  Current Loss: 0.0008333660662174225 and previously recorded Min Loss 0.0007454470614902675\n",
            "Iteration: 82/2070  Current Loss: 0.0010070678545162082 and previously recorded Min Loss 0.0007454470614902675\n",
            "Iteration: 83/2080  Current Loss: 0.0008315109880641103 and previously recorded Min Loss 0.0007454470614902675\n",
            "Iteration: 83/2090  Current Loss: 0.0007748528150841594 and previously recorded Min Loss 0.0007454470614902675\n",
            "Iteration: 83/2100  Current Loss: 0.0008137351251207292 and previously recorded Min Loss 0.0007454470614902675\n",
            "Iteration: 84/2110  Current Loss: 0.0007299263379536569 and previously recorded Min Loss 0.0007454470614902675\n",
            " Model saved\n",
            "Iteration: 84/2120  Current Loss: 0.0010664343135431409 and previously recorded Min Loss 0.0007299263379536569\n",
            "Iteration: 85/2130  Current Loss: 0.0011644779006019235 and previously recorded Min Loss 0.0007299263379536569\n",
            "Iteration: 85/2140  Current Loss: 0.0026603464502841234 and previously recorded Min Loss 0.0007299263379536569\n",
            "Iteration: 85/2150  Current Loss: 0.0015950414817780256 and previously recorded Min Loss 0.0007299263379536569\n",
            "Iteration: 86/2160  Current Loss: 0.0011587972985580564 and previously recorded Min Loss 0.0007299263379536569\n",
            "Iteration: 86/2170  Current Loss: 0.001242689904756844 and previously recorded Min Loss 0.0007299263379536569\n",
            "Iteration: 87/2180  Current Loss: 0.00111505133099854 and previously recorded Min Loss 0.0007299263379536569\n",
            "Iteration: 87/2190  Current Loss: 0.0010795897105708718 and previously recorded Min Loss 0.0007299263379536569\n",
            "Iteration: 87/2200  Current Loss: 0.0008263768395408988 and previously recorded Min Loss 0.0007299263379536569\n",
            "Iteration: 88/2210  Current Loss: 0.0007140379166230559 and previously recorded Min Loss 0.0007299263379536569\n",
            " Model saved\n",
            "Iteration: 88/2220  Current Loss: 0.0007461936911568046 and previously recorded Min Loss 0.0007140379166230559\n",
            "Iteration: 89/2230  Current Loss: 0.0007374542183242738 and previously recorded Min Loss 0.0007140379166230559\n",
            "Iteration: 89/2240  Current Loss: 0.001039178459905088 and previously recorded Min Loss 0.0007140379166230559\n",
            "Iteration: 89/2250  Current Loss: 0.0008069353643804789 and previously recorded Min Loss 0.0007140379166230559\n",
            "Iteration: 90/2260  Current Loss: 0.0007131771417334676 and previously recorded Min Loss 0.0007140379166230559\n",
            " Model saved\n",
            "Iteration: 90/2270  Current Loss: 0.000934399722609669 and previously recorded Min Loss 0.0007131771417334676\n",
            "Iteration: 91/2280  Current Loss: 0.0008056400693021715 and previously recorded Min Loss 0.0007131771417334676\n",
            "Iteration: 91/2290  Current Loss: 0.0007954226457513869 and previously recorded Min Loss 0.0007131771417334676\n",
            "Iteration: 91/2300  Current Loss: 0.0007849952671676874 and previously recorded Min Loss 0.0007131771417334676\n",
            "Iteration: 92/2310  Current Loss: 0.000725581543520093 and previously recorded Min Loss 0.0007131771417334676\n",
            "Iteration: 92/2320  Current Loss: 0.000756802735850215 and previously recorded Min Loss 0.0007131771417334676\n",
            "Iteration: 93/2330  Current Loss: 0.0007828379748389125 and previously recorded Min Loss 0.0007131771417334676\n",
            "Iteration: 93/2340  Current Loss: 0.0009624277590774 and previously recorded Min Loss 0.0007131771417334676\n",
            "Iteration: 93/2350  Current Loss: 0.0008454355993308127 and previously recorded Min Loss 0.0007131771417334676\n",
            "Iteration: 94/2360  Current Loss: 0.0008577816188335419 and previously recorded Min Loss 0.0007131771417334676\n",
            "Iteration: 94/2370  Current Loss: 0.0011133236112073064 and previously recorded Min Loss 0.0007131771417334676\n",
            "Iteration: 95/2380  Current Loss: 0.0010940280044451356 and previously recorded Min Loss 0.0007131771417334676\n",
            "Iteration: 95/2390  Current Loss: 0.0008901781984604895 and previously recorded Min Loss 0.0007131771417334676\n",
            "Iteration: 95/2400  Current Loss: 0.0009562053019180894 and previously recorded Min Loss 0.0007131771417334676\n",
            "Iteration: 96/2410  Current Loss: 0.0007022600038908422 and previously recorded Min Loss 0.0007131771417334676\n",
            " Model saved\n",
            "Iteration: 96/2420  Current Loss: 0.0006845708121545613 and previously recorded Min Loss 0.0007022600038908422\n",
            " Model saved\n",
            "Iteration: 97/2430  Current Loss: 0.0007849283283576369 and previously recorded Min Loss 0.0006845708121545613\n",
            "Iteration: 97/2440  Current Loss: 0.0007769124349579215 and previously recorded Min Loss 0.0006845708121545613\n",
            "Iteration: 97/2450  Current Loss: 0.0006972362170927227 and previously recorded Min Loss 0.0006845708121545613\n",
            "Iteration: 98/2460  Current Loss: 0.0006932661635801196 and previously recorded Min Loss 0.0006845708121545613\n",
            "Iteration: 98/2470  Current Loss: 0.0010000590700656176 and previously recorded Min Loss 0.0006845708121545613\n",
            "Iteration: 99/2480  Current Loss: 0.0006978533347137272 and previously recorded Min Loss 0.0006845708121545613\n",
            "Iteration: 99/2490  Current Loss: 0.0006992834969423711 and previously recorded Min Loss 0.0006845708121545613\n",
            "Iteration: 99/2500  Current Loss: 0.0007655654335394502 and previously recorded Min Loss 0.0006845708121545613\n",
            "Iteration: 100/2510  Current Loss: 0.0008449199376627803 and previously recorded Min Loss 0.0006845708121545613\n",
            "Iteration: 100/2520  Current Loss: 0.0009693866013549268 and previously recorded Min Loss 0.0006845708121545613\n",
            "Iteration: 101/2530  Current Loss: 0.000758612877689302 and previously recorded Min Loss 0.0006845708121545613\n",
            "Iteration: 101/2540  Current Loss: 0.0010284344898536801 and previously recorded Min Loss 0.0006845708121545613\n",
            "Iteration: 101/2550  Current Loss: 0.001030874322168529 and previously recorded Min Loss 0.0006845708121545613\n",
            "Iteration: 102/2560  Current Loss: 0.0008156542317010462 and previously recorded Min Loss 0.0006845708121545613\n",
            "Iteration: 102/2570  Current Loss: 0.0008826710982248187 and previously recorded Min Loss 0.0006845708121545613\n",
            "Iteration: 103/2580  Current Loss: 0.0008199232397601008 and previously recorded Min Loss 0.0006845708121545613\n",
            "Iteration: 103/2590  Current Loss: 0.0008250559912994504 and previously recorded Min Loss 0.0006845708121545613\n",
            "Iteration: 103/2600  Current Loss: 0.0007947559934109449 and previously recorded Min Loss 0.0006845708121545613\n",
            "Iteration: 104/2610  Current Loss: 0.0008548281039111316 and previously recorded Min Loss 0.0006845708121545613\n",
            "Iteration: 104/2620  Current Loss: 0.0007023562793619931 and previously recorded Min Loss 0.0006845708121545613\n",
            "Iteration: 105/2630  Current Loss: 0.0007633635541424155 and previously recorded Min Loss 0.0006845708121545613\n",
            "Iteration: 105/2640  Current Loss: 0.000808762270025909 and previously recorded Min Loss 0.0006845708121545613\n",
            "Iteration: 105/2650  Current Loss: 0.000847180257551372 and previously recorded Min Loss 0.0006845708121545613\n",
            "Iteration: 106/2660  Current Loss: 0.0007233352516777813 and previously recorded Min Loss 0.0006845708121545613\n",
            "Iteration: 106/2670  Current Loss: 0.0007953333551995456 and previously recorded Min Loss 0.0006845708121545613\n",
            "Iteration: 107/2680  Current Loss: 0.0010677435202524066 and previously recorded Min Loss 0.0006845708121545613\n",
            "Iteration: 107/2690  Current Loss: 0.0008434324408881366 and previously recorded Min Loss 0.0006845708121545613\n",
            "Iteration: 107/2700  Current Loss: 0.0007115154876373708 and previously recorded Min Loss 0.0006845708121545613\n",
            "Iteration: 108/2710  Current Loss: 0.0006764955469407141 and previously recorded Min Loss 0.0006845708121545613\n",
            " Model saved\n",
            "Iteration: 108/2720  Current Loss: 0.0006660774815827608 and previously recorded Min Loss 0.0006764955469407141\n",
            " Model saved\n",
            "Iteration: 109/2730  Current Loss: 0.0007817102014087141 and previously recorded Min Loss 0.0006660774815827608\n",
            "Iteration: 109/2740  Current Loss: 0.0006817541434429586 and previously recorded Min Loss 0.0006660774815827608\n",
            "Iteration: 109/2750  Current Loss: 0.0006975472206249833 and previously recorded Min Loss 0.0006660774815827608\n",
            "Iteration: 110/2760  Current Loss: 0.0006750691100023687 and previously recorded Min Loss 0.0006660774815827608\n",
            "Iteration: 110/2770  Current Loss: 0.0006715840427204967 and previously recorded Min Loss 0.0006660774815827608\n",
            "Iteration: 111/2780  Current Loss: 0.000728268176317215 and previously recorded Min Loss 0.0006660774815827608\n",
            "Iteration: 111/2790  Current Loss: 0.0006716865464113653 and previously recorded Min Loss 0.0006660774815827608\n",
            "Iteration: 111/2800  Current Loss: 0.0007180696120485663 and previously recorded Min Loss 0.0006660774815827608\n",
            "Iteration: 112/2810  Current Loss: 0.0007411488913930953 and previously recorded Min Loss 0.0006660774815827608\n",
            "Iteration: 112/2820  Current Loss: 0.0008182954625226557 and previously recorded Min Loss 0.0006660774815827608\n",
            "Iteration: 113/2830  Current Loss: 0.0015910888323560357 and previously recorded Min Loss 0.0006660774815827608\n",
            "Iteration: 113/2840  Current Loss: 0.0007838797173462808 and previously recorded Min Loss 0.0006660774815827608\n",
            "Iteration: 113/2850  Current Loss: 0.001037860056385398 and previously recorded Min Loss 0.0006660774815827608\n",
            "Iteration: 114/2860  Current Loss: 0.0008784721721895039 and previously recorded Min Loss 0.0006660774815827608\n",
            "Iteration: 114/2870  Current Loss: 0.000836419640108943 and previously recorded Min Loss 0.0006660774815827608\n",
            "Iteration: 115/2880  Current Loss: 0.0010998708894476295 and previously recorded Min Loss 0.0006660774815827608\n",
            "Iteration: 115/2890  Current Loss: 0.0006930133677087724 and previously recorded Min Loss 0.0006660774815827608\n",
            "Iteration: 115/2900  Current Loss: 0.0009051653905771673 and previously recorded Min Loss 0.0006660774815827608\n",
            "Iteration: 116/2910  Current Loss: 0.0006893379613757133 and previously recorded Min Loss 0.0006660774815827608\n",
            "Iteration: 116/2920  Current Loss: 0.0006332978955470026 and previously recorded Min Loss 0.0006660774815827608\n",
            " Model saved\n",
            "Iteration: 117/2930  Current Loss: 0.000908204703591764 and previously recorded Min Loss 0.0006332978955470026\n",
            "Iteration: 117/2940  Current Loss: 0.0006281938985921443 and previously recorded Min Loss 0.0006332978955470026\n",
            " Model saved\n",
            "Iteration: 117/2950  Current Loss: 0.0007744767935946584 and previously recorded Min Loss 0.0006281938985921443\n",
            "Iteration: 118/2960  Current Loss: 0.0007500742212869227 and previously recorded Min Loss 0.0006281938985921443\n",
            "Iteration: 118/2970  Current Loss: 0.0007554239709861577 and previously recorded Min Loss 0.0006281938985921443\n",
            "Iteration: 119/2980  Current Loss: 0.000903861247934401 and previously recorded Min Loss 0.0006281938985921443\n",
            "Iteration: 119/2990  Current Loss: 0.0007237660465762019 and previously recorded Min Loss 0.0006281938985921443\n",
            "Iteration: 119/3000  Current Loss: 0.0007189699681475759 and previously recorded Min Loss 0.0006281938985921443\n",
            "Iteration: 120/3010  Current Loss: 0.0006916394922882318 and previously recorded Min Loss 0.0006281938985921443\n",
            "Iteration: 120/3020  Current Loss: 0.0007407459779642522 and previously recorded Min Loss 0.0006281938985921443\n",
            "Iteration: 121/3030  Current Loss: 0.0006580256158486009 and previously recorded Min Loss 0.0006281938985921443\n",
            "Iteration: 121/3040  Current Loss: 0.0006929271621629596 and previously recorded Min Loss 0.0006281938985921443\n",
            "Iteration: 121/3050  Current Loss: 0.0007778207072988153 and previously recorded Min Loss 0.0006281938985921443\n",
            "Iteration: 122/3060  Current Loss: 0.000835548504255712 and previously recorded Min Loss 0.0006281938985921443\n",
            "Iteration: 122/3070  Current Loss: 0.000986000755801797 and previously recorded Min Loss 0.0006281938985921443\n",
            "Iteration: 123/3080  Current Loss: 0.0006412474904209375 and previously recorded Min Loss 0.0006281938985921443\n",
            "Iteration: 123/3090  Current Loss: 0.0007034912705421448 and previously recorded Min Loss 0.0006281938985921443\n",
            "Iteration: 123/3100  Current Loss: 0.0006722497055307031 and previously recorded Min Loss 0.0006281938985921443\n",
            "Iteration: 124/3110  Current Loss: 0.0008137756958603859 and previously recorded Min Loss 0.0006281938985921443\n",
            "Iteration: 124/3120  Current Loss: 0.0007301485165953636 and previously recorded Min Loss 0.0006281938985921443\n",
            "Iteration: 125/3130  Current Loss: 0.0007181915570981801 and previously recorded Min Loss 0.0006281938985921443\n",
            "Iteration: 125/3140  Current Loss: 0.001699507818557322 and previously recorded Min Loss 0.0006281938985921443\n",
            "Iteration: 125/3150  Current Loss: 0.0010397655423730612 and previously recorded Min Loss 0.0006281938985921443\n",
            "Iteration: 126/3160  Current Loss: 0.0010005402145907283 and previously recorded Min Loss 0.0006281938985921443\n",
            "Iteration: 126/3170  Current Loss: 0.0009792569326236844 and previously recorded Min Loss 0.0006281938985921443\n",
            "Iteration: 127/3180  Current Loss: 0.0011026549618691206 and previously recorded Min Loss 0.0006281938985921443\n",
            "Iteration: 127/3190  Current Loss: 0.0009725714335218072 and previously recorded Min Loss 0.0006281938985921443\n",
            "Iteration: 127/3200  Current Loss: 0.0007170762401074171 and previously recorded Min Loss 0.0006281938985921443\n",
            "Iteration: 128/3210  Current Loss: 0.0007313911337405443 and previously recorded Min Loss 0.0006281938985921443\n",
            "Iteration: 128/3220  Current Loss: 0.0006900332518853247 and previously recorded Min Loss 0.0006281938985921443\n",
            "Iteration: 129/3230  Current Loss: 0.0006320645334199071 and previously recorded Min Loss 0.0006281938985921443\n",
            "Iteration: 129/3240  Current Loss: 0.0008293060818687081 and previously recorded Min Loss 0.0006281938985921443\n",
            "Iteration: 129/3250  Current Loss: 0.0006678012432530522 and previously recorded Min Loss 0.0006281938985921443\n",
            "Iteration: 130/3260  Current Loss: 0.000735573354177177 and previously recorded Min Loss 0.0006281938985921443\n",
            "Iteration: 130/3270  Current Loss: 0.0006264703697524965 and previously recorded Min Loss 0.0006281938985921443\n",
            " Model saved\n",
            "Iteration: 131/3280  Current Loss: 0.0006968078087083995 and previously recorded Min Loss 0.0006264703697524965\n",
            "Iteration: 131/3290  Current Loss: 0.0006733239279128611 and previously recorded Min Loss 0.0006264703697524965\n",
            "Iteration: 131/3300  Current Loss: 0.0007811703253537416 and previously recorded Min Loss 0.0006264703697524965\n",
            "Iteration: 132/3310  Current Loss: 0.000805365270934999 and previously recorded Min Loss 0.0006264703697524965\n",
            "Iteration: 132/3320  Current Loss: 0.0006352110067382455 and previously recorded Min Loss 0.0006264703697524965\n",
            "Iteration: 133/3330  Current Loss: 0.0009069100487977266 and previously recorded Min Loss 0.0006264703697524965\n",
            "Iteration: 133/3340  Current Loss: 0.0006520153256133199 and previously recorded Min Loss 0.0006264703697524965\n",
            "Iteration: 133/3350  Current Loss: 0.0006681450759060681 and previously recorded Min Loss 0.0006264703697524965\n",
            "Iteration: 134/3360  Current Loss: 0.0007658654358237982 and previously recorded Min Loss 0.0006264703697524965\n",
            "Iteration: 134/3370  Current Loss: 0.0007338555296882987 and previously recorded Min Loss 0.0006264703697524965\n",
            "Iteration: 135/3380  Current Loss: 0.0006288994918577373 and previously recorded Min Loss 0.0006264703697524965\n",
            "Iteration: 135/3390  Current Loss: 0.0006351002375595272 and previously recorded Min Loss 0.0006264703697524965\n",
            "Iteration: 135/3400  Current Loss: 0.0006119587924331427 and previously recorded Min Loss 0.0006264703697524965\n",
            " Model saved\n",
            "Iteration: 136/3410  Current Loss: 0.0009660994983278215 and previously recorded Min Loss 0.0006119587924331427\n",
            "Iteration: 136/3420  Current Loss: 0.001227295259013772 and previously recorded Min Loss 0.0006119587924331427\n",
            "Iteration: 137/3430  Current Loss: 0.0007378197624348104 and previously recorded Min Loss 0.0006119587924331427\n",
            "Iteration: 137/3440  Current Loss: 0.0006521987961605191 and previously recorded Min Loss 0.0006119587924331427\n",
            "Iteration: 137/3450  Current Loss: 0.0006423619342967868 and previously recorded Min Loss 0.0006119587924331427\n",
            "Iteration: 138/3460  Current Loss: 0.0006247744313441217 and previously recorded Min Loss 0.0006119587924331427\n",
            "Iteration: 138/3470  Current Loss: 0.000692634261213243 and previously recorded Min Loss 0.0006119587924331427\n",
            "Iteration: 139/3480  Current Loss: 0.0007375099812634289 and previously recorded Min Loss 0.0006119587924331427\n",
            "Iteration: 139/3490  Current Loss: 0.000654926523566246 and previously recorded Min Loss 0.0006119587924331427\n",
            "Iteration: 139/3500  Current Loss: 0.0006188676343299448 and previously recorded Min Loss 0.0006119587924331427\n",
            "Iteration: 140/3510  Current Loss: 0.0005964148440398276 and previously recorded Min Loss 0.0006119587924331427\n",
            " Model saved\n",
            "Iteration: 140/3520  Current Loss: 0.0006059064180590212 and previously recorded Min Loss 0.0005964148440398276\n",
            "Iteration: 141/3530  Current Loss: 0.0006638498743996024 and previously recorded Min Loss 0.0005964148440398276\n",
            "Iteration: 141/3540  Current Loss: 0.0006514051929116249 and previously recorded Min Loss 0.0005964148440398276\n",
            "Iteration: 141/3550  Current Loss: 0.0007352615939453244 and previously recorded Min Loss 0.0005964148440398276\n",
            "Iteration: 142/3560  Current Loss: 0.0006661057705059648 and previously recorded Min Loss 0.0005964148440398276\n",
            "Iteration: 142/3570  Current Loss: 0.0011250595562160015 and previously recorded Min Loss 0.0005964148440398276\n",
            "Iteration: 143/3580  Current Loss: 0.0008739611366763711 and previously recorded Min Loss 0.0005964148440398276\n",
            "Iteration: 143/3590  Current Loss: 0.0008511472842656076 and previously recorded Min Loss 0.0005964148440398276\n",
            "Iteration: 143/3600  Current Loss: 0.0008559305570088327 and previously recorded Min Loss 0.0005964148440398276\n",
            "Iteration: 144/3610  Current Loss: 0.0006837242399342358 and previously recorded Min Loss 0.0005964148440398276\n",
            "Iteration: 144/3620  Current Loss: 0.0009934830013662577 and previously recorded Min Loss 0.0005964148440398276\n",
            "Iteration: 145/3630  Current Loss: 0.0006490329978987575 and previously recorded Min Loss 0.0005964148440398276\n",
            "Iteration: 145/3640  Current Loss: 0.0007032818393781781 and previously recorded Min Loss 0.0005964148440398276\n",
            "Iteration: 145/3650  Current Loss: 0.0007894666050560772 and previously recorded Min Loss 0.0005964148440398276\n",
            "Iteration: 146/3660  Current Loss: 0.0007468042895197868 and previously recorded Min Loss 0.0005964148440398276\n",
            "Iteration: 146/3670  Current Loss: 0.0006898252759128809 and previously recorded Min Loss 0.0005964148440398276\n",
            "Iteration: 147/3680  Current Loss: 0.0006784802535548806 and previously recorded Min Loss 0.0005964148440398276\n",
            "Iteration: 147/3690  Current Loss: 0.0006489366060122848 and previously recorded Min Loss 0.0005964148440398276\n",
            "Iteration: 147/3700  Current Loss: 0.0007127114804461598 and previously recorded Min Loss 0.0005964148440398276\n",
            "Iteration: 148/3710  Current Loss: 0.0007345631020143628 and previously recorded Min Loss 0.0005964148440398276\n",
            "Iteration: 148/3720  Current Loss: 0.0006738236406818032 and previously recorded Min Loss 0.0005964148440398276\n",
            "Iteration: 149/3730  Current Loss: 0.0007197496597655118 and previously recorded Min Loss 0.0005964148440398276\n",
            "Iteration: 149/3740  Current Loss: 0.0009329707827419043 and previously recorded Min Loss 0.0005964148440398276\n",
            "Iteration: 149/3750  Current Loss: 0.0008256201399490237 and previously recorded Min Loss 0.0005964148440398276\n",
            "Iteration: 150/3760  Current Loss: 0.000947270542383194 and previously recorded Min Loss 0.0005964148440398276\n",
            "Iteration: 150/3770  Current Loss: 0.0006902074092067778 and previously recorded Min Loss 0.0005964148440398276\n",
            "Iteration: 151/3780  Current Loss: 0.0008461950346827507 and previously recorded Min Loss 0.0005964148440398276\n",
            "Iteration: 151/3790  Current Loss: 0.0007225160370580852 and previously recorded Min Loss 0.0005964148440398276\n",
            "Iteration: 151/3800  Current Loss: 0.0006300312816165388 and previously recorded Min Loss 0.0005964148440398276\n",
            "Iteration: 152/3810  Current Loss: 0.0007104336400516331 and previously recorded Min Loss 0.0005964148440398276\n",
            "Iteration: 152/3820  Current Loss: 0.0006080460152588785 and previously recorded Min Loss 0.0005964148440398276\n",
            "Iteration: 153/3830  Current Loss: 0.0006478781579062343 and previously recorded Min Loss 0.0005964148440398276\n",
            "Iteration: 153/3840  Current Loss: 0.0006867056945338845 and previously recorded Min Loss 0.0005964148440398276\n",
            "Iteration: 153/3850  Current Loss: 0.0007687111501581967 and previously recorded Min Loss 0.0005964148440398276\n",
            "Iteration: 154/3860  Current Loss: 0.0007182746776379645 and previously recorded Min Loss 0.0005964148440398276\n",
            "Iteration: 154/3870  Current Loss: 0.0006147577660158277 and previously recorded Min Loss 0.0005964148440398276\n",
            "Iteration: 155/3880  Current Loss: 0.0011038824450224638 and previously recorded Min Loss 0.0005964148440398276\n",
            "Iteration: 155/3890  Current Loss: 0.0015000381972640753 and previously recorded Min Loss 0.0005964148440398276\n",
            "Iteration: 155/3900  Current Loss: 0.0009725961135700345 and previously recorded Min Loss 0.0005964148440398276\n",
            "Iteration: 156/3910  Current Loss: 0.0006481760647147894 and previously recorded Min Loss 0.0005964148440398276\n",
            "Iteration: 156/3920  Current Loss: 0.0005986978067085147 and previously recorded Min Loss 0.0005964148440398276\n",
            "Iteration: 157/3930  Current Loss: 0.0006103640771470964 and previously recorded Min Loss 0.0005964148440398276\n",
            "Iteration: 157/3940  Current Loss: 0.0007544320542365313 and previously recorded Min Loss 0.0005964148440398276\n",
            "Iteration: 157/3950  Current Loss: 0.0010353021789342165 and previously recorded Min Loss 0.0005964148440398276\n",
            "Iteration: 158/3960  Current Loss: 0.0008593499660491943 and previously recorded Min Loss 0.0005964148440398276\n",
            "Iteration: 158/3970  Current Loss: 0.000637040997389704 and previously recorded Min Loss 0.0005964148440398276\n",
            "Iteration: 159/3980  Current Loss: 0.0007566673448309302 and previously recorded Min Loss 0.0005964148440398276\n",
            "Iteration: 159/3990  Current Loss: 0.0006080841994844377 and previously recorded Min Loss 0.0005964148440398276\n",
            "Iteration: 159/4000  Current Loss: 0.0006918023573234677 and previously recorded Min Loss 0.0005964148440398276\n",
            "Iteration: 160/4010  Current Loss: 0.000633501447737217 and previously recorded Min Loss 0.0005964148440398276\n",
            "Iteration: 160/4020  Current Loss: 0.0006206747493706644 and previously recorded Min Loss 0.0005964148440398276\n",
            "Iteration: 161/4030  Current Loss: 0.00056198809761554 and previously recorded Min Loss 0.0005964148440398276\n",
            " Model saved\n",
            "Iteration: 161/4040  Current Loss: 0.0006550821126438677 and previously recorded Min Loss 0.00056198809761554\n",
            "Iteration: 161/4050  Current Loss: 0.0007292794762179255 and previously recorded Min Loss 0.00056198809761554\n",
            "Iteration: 162/4060  Current Loss: 0.0006234828615561128 and previously recorded Min Loss 0.00056198809761554\n",
            "Iteration: 162/4070  Current Loss: 0.0006163451471365988 and previously recorded Min Loss 0.00056198809761554\n",
            "Iteration: 163/4080  Current Loss: 0.0007259560516104102 and previously recorded Min Loss 0.00056198809761554\n",
            "Iteration: 163/4090  Current Loss: 0.0009674926986917853 and previously recorded Min Loss 0.00056198809761554\n",
            "Iteration: 163/4100  Current Loss: 0.0006722428370267153 and previously recorded Min Loss 0.00056198809761554\n",
            "Iteration: 164/4110  Current Loss: 0.0006089757080189884 and previously recorded Min Loss 0.00056198809761554\n",
            "Iteration: 164/4120  Current Loss: 0.0007741007138974965 and previously recorded Min Loss 0.00056198809761554\n",
            "Iteration: 165/4130  Current Loss: 0.0006743255653418601 and previously recorded Min Loss 0.00056198809761554\n",
            "Iteration: 165/4140  Current Loss: 0.0008893680642358959 and previously recorded Min Loss 0.00056198809761554\n",
            "Iteration: 165/4150  Current Loss: 0.0006574137951247394 and previously recorded Min Loss 0.00056198809761554\n",
            "Iteration: 166/4160  Current Loss: 0.0006950214155949652 and previously recorded Min Loss 0.00056198809761554\n",
            "Iteration: 166/4170  Current Loss: 0.0007480322965420783 and previously recorded Min Loss 0.00056198809761554\n",
            "Iteration: 167/4180  Current Loss: 0.0006083581247366965 and previously recorded Min Loss 0.00056198809761554\n",
            "Iteration: 167/4190  Current Loss: 0.0007320906734094024 and previously recorded Min Loss 0.00056198809761554\n",
            "Iteration: 167/4200  Current Loss: 0.0006424202001653612 and previously recorded Min Loss 0.00056198809761554\n",
            "Iteration: 168/4210  Current Loss: 0.0006349760806187987 and previously recorded Min Loss 0.00056198809761554\n",
            "Iteration: 168/4220  Current Loss: 0.0006034544203430414 and previously recorded Min Loss 0.00056198809761554\n",
            "Iteration: 169/4230  Current Loss: 0.0006264882395043969 and previously recorded Min Loss 0.00056198809761554\n",
            "Iteration: 169/4240  Current Loss: 0.0005928821046836674 and previously recorded Min Loss 0.00056198809761554\n",
            "Iteration: 169/4250  Current Loss: 0.0006084175547584891 and previously recorded Min Loss 0.00056198809761554\n",
            "Iteration: 170/4260  Current Loss: 0.0006128604873083532 and previously recorded Min Loss 0.00056198809761554\n",
            "Iteration: 170/4270  Current Loss: 0.0005890178726986051 and previously recorded Min Loss 0.00056198809761554\n",
            "Iteration: 171/4280  Current Loss: 0.000626309891231358 and previously recorded Min Loss 0.00056198809761554\n",
            "Iteration: 171/4290  Current Loss: 0.0006258889334276319 and previously recorded Min Loss 0.00056198809761554\n",
            "Iteration: 171/4300  Current Loss: 0.0005845363484695554 and previously recorded Min Loss 0.00056198809761554\n",
            "Iteration: 172/4310  Current Loss: 0.0007957881316542625 and previously recorded Min Loss 0.00056198809761554\n",
            "Iteration: 172/4320  Current Loss: 0.0007661996060051024 and previously recorded Min Loss 0.00056198809761554\n",
            "Iteration: 173/4330  Current Loss: 0.00065375812118873 and previously recorded Min Loss 0.00056198809761554\n",
            "Iteration: 173/4340  Current Loss: 0.0005963350413367152 and previously recorded Min Loss 0.00056198809761554\n",
            "Iteration: 173/4350  Current Loss: 0.001007461454719305 and previously recorded Min Loss 0.00056198809761554\n",
            "Iteration: 174/4360  Current Loss: 0.0006402750732377172 and previously recorded Min Loss 0.00056198809761554\n",
            "Iteration: 174/4370  Current Loss: 0.0005926499143242836 and previously recorded Min Loss 0.00056198809761554\n",
            "Iteration: 175/4380  Current Loss: 0.0008460567914880812 and previously recorded Min Loss 0.00056198809761554\n",
            "Iteration: 175/4390  Current Loss: 0.0012615086743608117 and previously recorded Min Loss 0.00056198809761554\n",
            "Iteration: 175/4400  Current Loss: 0.0009370577172376215 and previously recorded Min Loss 0.00056198809761554\n",
            "Iteration: 176/4410  Current Loss: 0.0018300216179341078 and previously recorded Min Loss 0.00056198809761554\n",
            "Iteration: 176/4420  Current Loss: 0.001984776696190238 and previously recorded Min Loss 0.00056198809761554\n",
            "Iteration: 177/4430  Current Loss: 0.001079295645467937 and previously recorded Min Loss 0.00056198809761554\n",
            "Iteration: 177/4440  Current Loss: 0.0010571948951110244 and previously recorded Min Loss 0.00056198809761554\n",
            "Iteration: 177/4450  Current Loss: 0.0009807629976421595 and previously recorded Min Loss 0.00056198809761554\n",
            "Iteration: 178/4460  Current Loss: 0.0010068808915093541 and previously recorded Min Loss 0.00056198809761554\n",
            "Iteration: 178/4470  Current Loss: 0.0012238918570801616 and previously recorded Min Loss 0.00056198809761554\n",
            "Iteration: 179/4480  Current Loss: 0.0011752289719879627 and previously recorded Min Loss 0.00056198809761554\n",
            "Iteration: 179/4490  Current Loss: 0.0008498697425238788 and previously recorded Min Loss 0.00056198809761554\n",
            "Iteration: 179/4500  Current Loss: 0.0005951811908744276 and previously recorded Min Loss 0.00056198809761554\n",
            "Iteration: 180/4510  Current Loss: 0.0005876246141269803 and previously recorded Min Loss 0.00056198809761554\n",
            "Iteration: 180/4520  Current Loss: 0.0006051883683539927 and previously recorded Min Loss 0.00056198809761554\n",
            "Iteration: 181/4530  Current Loss: 0.0006455279653891921 and previously recorded Min Loss 0.00056198809761554\n",
            "Iteration: 181/4540  Current Loss: 0.0005718148313462734 and previously recorded Min Loss 0.00056198809761554\n",
            "Iteration: 181/4550  Current Loss: 0.0006187241524457932 and previously recorded Min Loss 0.00056198809761554\n",
            "Iteration: 182/4560  Current Loss: 0.0005939275724813342 and previously recorded Min Loss 0.00056198809761554\n",
            "Iteration: 182/4570  Current Loss: 0.0005986245814710855 and previously recorded Min Loss 0.00056198809761554\n",
            "Iteration: 183/4580  Current Loss: 0.0006448341882787645 and previously recorded Min Loss 0.00056198809761554\n",
            "Iteration: 183/4590  Current Loss: 0.0006824436713941395 and previously recorded Min Loss 0.00056198809761554\n",
            "Iteration: 183/4600  Current Loss: 0.0006394926458597183 and previously recorded Min Loss 0.00056198809761554\n",
            "Iteration: 184/4610  Current Loss: 0.0006237986963242292 and previously recorded Min Loss 0.00056198809761554\n",
            "Iteration: 184/4620  Current Loss: 0.0005945403827354312 and previously recorded Min Loss 0.00056198809761554\n",
            "Iteration: 185/4630  Current Loss: 0.0006280217785388231 and previously recorded Min Loss 0.00056198809761554\n",
            "Iteration: 185/4640  Current Loss: 0.0006035511614754796 and previously recorded Min Loss 0.00056198809761554\n",
            "Iteration: 185/4650  Current Loss: 0.0006890426157042384 and previously recorded Min Loss 0.00056198809761554\n",
            "Iteration: 186/4660  Current Loss: 0.0006548261735588312 and previously recorded Min Loss 0.00056198809761554\n",
            "Iteration: 186/4670  Current Loss: 0.0006169878179207444 and previously recorded Min Loss 0.00056198809761554\n",
            "Iteration: 187/4680  Current Loss: 0.0006051608943380415 and previously recorded Min Loss 0.00056198809761554\n",
            "Iteration: 187/4690  Current Loss: 0.0005826622364111245 and previously recorded Min Loss 0.00056198809761554\n",
            "Iteration: 187/4700  Current Loss: 0.0005768577684648335 and previously recorded Min Loss 0.00056198809761554\n",
            "Iteration: 188/4710  Current Loss: 0.000581922591663897 and previously recorded Min Loss 0.00056198809761554\n",
            "Iteration: 188/4720  Current Loss: 0.000610094633884728 and previously recorded Min Loss 0.00056198809761554\n",
            "Iteration: 189/4730  Current Loss: 0.0006068615475669503 and previously recorded Min Loss 0.00056198809761554\n",
            "Iteration: 189/4740  Current Loss: 0.000579850107897073 and previously recorded Min Loss 0.00056198809761554\n",
            "Iteration: 189/4750  Current Loss: 0.0007046769023872912 and previously recorded Min Loss 0.00056198809761554\n",
            "Iteration: 190/4760  Current Loss: 0.000603188353125006 and previously recorded Min Loss 0.00056198809761554\n",
            "Iteration: 190/4770  Current Loss: 0.0006182331126183271 and previously recorded Min Loss 0.00056198809761554\n",
            "Iteration: 191/4780  Current Loss: 0.0005841759266331792 and previously recorded Min Loss 0.00056198809761554\n",
            "Iteration: 191/4790  Current Loss: 0.0005631823441945016 and previously recorded Min Loss 0.00056198809761554\n",
            "Iteration: 191/4800  Current Loss: 0.0005835854099132121 and previously recorded Min Loss 0.00056198809761554\n",
            "Iteration: 192/4810  Current Loss: 0.0005858055083081126 and previously recorded Min Loss 0.00056198809761554\n",
            "Iteration: 192/4820  Current Loss: 0.0006353622884489596 and previously recorded Min Loss 0.00056198809761554\n",
            "Iteration: 193/4830  Current Loss: 0.0005888546584174037 and previously recorded Min Loss 0.00056198809761554\n",
            "Iteration: 193/4840  Current Loss: 0.0007335325935855508 and previously recorded Min Loss 0.00056198809761554\n",
            "Iteration: 193/4850  Current Loss: 0.0008162847370840609 and previously recorded Min Loss 0.00056198809761554\n",
            "Iteration: 194/4860  Current Loss: 0.0007766738999634981 and previously recorded Min Loss 0.00056198809761554\n",
            "Iteration: 194/4870  Current Loss: 0.0007934816530905664 and previously recorded Min Loss 0.00056198809761554\n",
            "Iteration: 195/4880  Current Loss: 0.00099094002507627 and previously recorded Min Loss 0.00056198809761554\n",
            "Iteration: 195/4890  Current Loss: 0.000714880705345422 and previously recorded Min Loss 0.00056198809761554\n",
            "Iteration: 195/4900  Current Loss: 0.0008199579315260053 and previously recorded Min Loss 0.00056198809761554\n",
            "Iteration: 196/4910  Current Loss: 0.0006984988576732576 and previously recorded Min Loss 0.00056198809761554\n",
            "Iteration: 196/4920  Current Loss: 0.0009384844452142715 and previously recorded Min Loss 0.00056198809761554\n",
            "Iteration: 197/4930  Current Loss: 0.0006978031015023589 and previously recorded Min Loss 0.00056198809761554\n",
            "Iteration: 197/4940  Current Loss: 0.0006905053160153329 and previously recorded Min Loss 0.00056198809761554\n",
            "Iteration: 197/4950  Current Loss: 0.0005994834355078638 and previously recorded Min Loss 0.00056198809761554\n",
            "Iteration: 198/4960  Current Loss: 0.0007040500058792531 and previously recorded Min Loss 0.00056198809761554\n",
            "Iteration: 198/4970  Current Loss: 0.0006603726069442928 and previously recorded Min Loss 0.00056198809761554\n",
            "Iteration: 199/4980  Current Loss: 0.0006512001855298877 and previously recorded Min Loss 0.00056198809761554\n",
            "Iteration: 199/4990  Current Loss: 0.000616215169429779 and previously recorded Min Loss 0.00056198809761554\n",
            "Iteration: 199/5000  Current Loss: 0.0006797435926273465 and previously recorded Min Loss 0.00056198809761554\n",
            "Iteration: 200/5010  Current Loss: 0.0007193947676569223 and previously recorded Min Loss 0.00056198809761554\n",
            "Iteration: 200/5020  Current Loss: 0.0008138763369061053 and previously recorded Min Loss 0.00056198809761554\n",
            "Iteration: 201/5030  Current Loss: 0.0006423854501917958 and previously recorded Min Loss 0.00056198809761554\n",
            "Iteration: 201/5040  Current Loss: 0.0006589446566067636 and previously recorded Min Loss 0.00056198809761554\n",
            "Iteration: 201/5050  Current Loss: 0.0006292753387242556 and previously recorded Min Loss 0.00056198809761554\n",
            "Iteration: 202/5060  Current Loss: 0.0005817881319671869 and previously recorded Min Loss 0.00056198809761554\n",
            "Iteration: 202/5070  Current Loss: 0.0005756623577326536 and previously recorded Min Loss 0.00056198809761554\n",
            "Iteration: 203/5080  Current Loss: 0.0007990925805643201 and previously recorded Min Loss 0.00056198809761554\n",
            "Iteration: 203/5090  Current Loss: 0.0006891169468872249 and previously recorded Min Loss 0.00056198809761554\n",
            "Iteration: 203/5100  Current Loss: 0.0005608760984614491 and previously recorded Min Loss 0.00056198809761554\n",
            " Model saved\n",
            "Iteration: 204/5110  Current Loss: 0.0006016219267621636 and previously recorded Min Loss 0.0005608760984614491\n",
            "Iteration: 204/5120  Current Loss: 0.000659950717817992 and previously recorded Min Loss 0.0005608760984614491\n",
            "Iteration: 205/5130  Current Loss: 0.0005918037495575845 and previously recorded Min Loss 0.0005608760984614491\n",
            "Iteration: 205/5140  Current Loss: 0.0006107702502049506 and previously recorded Min Loss 0.0005608760984614491\n",
            "Iteration: 205/5150  Current Loss: 0.00070843321736902 and previously recorded Min Loss 0.0005608760984614491\n",
            "Iteration: 206/5160  Current Loss: 0.0006283483235165477 and previously recorded Min Loss 0.0005608760984614491\n",
            "Iteration: 206/5170  Current Loss: 0.0007763985195197165 and previously recorded Min Loss 0.0005608760984614491\n",
            "Iteration: 207/5180  Current Loss: 0.0006155439768917859 and previously recorded Min Loss 0.0005608760984614491\n",
            "Iteration: 207/5190  Current Loss: 0.0006121898768469691 and previously recorded Min Loss 0.0005608760984614491\n",
            "Iteration: 207/5200  Current Loss: 0.0006405272870324552 and previously recorded Min Loss 0.0005608760984614491\n",
            "Iteration: 208/5210  Current Loss: 0.000649786030407995 and previously recorded Min Loss 0.0005608760984614491\n",
            "Iteration: 208/5220  Current Loss: 0.0006199810886755586 and previously recorded Min Loss 0.0005608760984614491\n",
            "Iteration: 209/5230  Current Loss: 0.0006011249497532845 and previously recorded Min Loss 0.0005608760984614491\n",
            "Iteration: 209/5240  Current Loss: 0.0006174956797622144 and previously recorded Min Loss 0.0005608760984614491\n",
            "Iteration: 209/5250  Current Loss: 0.0008304233197122812 and previously recorded Min Loss 0.0005608760984614491\n",
            "Iteration: 210/5260  Current Loss: 0.0006860283901914954 and previously recorded Min Loss 0.0005608760984614491\n",
            "Iteration: 210/5270  Current Loss: 0.0006424940074793994 and previously recorded Min Loss 0.0005608760984614491\n",
            "Iteration: 211/5280  Current Loss: 0.0006353182252496481 and previously recorded Min Loss 0.0005608760984614491\n",
            "Iteration: 211/5290  Current Loss: 0.0005437019281089306 and previously recorded Min Loss 0.0005608760984614491\n",
            " Model saved\n",
            "Iteration: 211/5300  Current Loss: 0.0008452671463601291 and previously recorded Min Loss 0.0005437019281089306\n",
            "Iteration: 212/5310  Current Loss: 0.0006404360174201429 and previously recorded Min Loss 0.0005437019281089306\n",
            "Iteration: 212/5320  Current Loss: 0.0008108296897262335 and previously recorded Min Loss 0.0005437019281089306\n",
            "Iteration: 213/5330  Current Loss: 0.0006736128125339746 and previously recorded Min Loss 0.0005437019281089306\n",
            "Iteration: 213/5340  Current Loss: 0.00062782276654616 and previously recorded Min Loss 0.0005437019281089306\n",
            "Iteration: 213/5350  Current Loss: 0.0013513365993276238 and previously recorded Min Loss 0.0005437019281089306\n",
            "Iteration: 214/5360  Current Loss: 0.0007818885496817529 and previously recorded Min Loss 0.0005437019281089306\n",
            "Iteration: 214/5370  Current Loss: 0.000955103081651032 and previously recorded Min Loss 0.0005437019281089306\n",
            "Iteration: 215/5380  Current Loss: 0.0006605553207919002 and previously recorded Min Loss 0.0005437019281089306\n",
            "Iteration: 215/5390  Current Loss: 0.0008647675858810544 and previously recorded Min Loss 0.0005437019281089306\n",
            "Iteration: 215/5400  Current Loss: 0.0006413683877326548 and previously recorded Min Loss 0.0005437019281089306\n",
            "Iteration: 216/5410  Current Loss: 0.0006268688011914492 and previously recorded Min Loss 0.0005437019281089306\n",
            "Iteration: 216/5420  Current Loss: 0.000586915120948106 and previously recorded Min Loss 0.0005437019281089306\n",
            "Iteration: 217/5430  Current Loss: 0.0006109316600486636 and previously recorded Min Loss 0.0005437019281089306\n",
            "Iteration: 217/5440  Current Loss: 0.0005460272077471018 and previously recorded Min Loss 0.0005437019281089306\n",
            "Iteration: 217/5450  Current Loss: 0.0005538322729989886 and previously recorded Min Loss 0.0005437019281089306\n",
            "Iteration: 218/5460  Current Loss: 0.000581516302190721 and previously recorded Min Loss 0.0005437019281089306\n",
            "Iteration: 218/5470  Current Loss: 0.0006588961114175618 and previously recorded Min Loss 0.0005437019281089306\n",
            "Iteration: 219/5480  Current Loss: 0.0006645657122135162 and previously recorded Min Loss 0.0005437019281089306\n",
            "Iteration: 219/5490  Current Loss: 0.0007449332624673843 and previously recorded Min Loss 0.0005437019281089306\n",
            "Iteration: 219/5500  Current Loss: 0.0006170825799927115 and previously recorded Min Loss 0.0005437019281089306\n",
            "Iteration: 220/5510  Current Loss: 0.000624708947725594 and previously recorded Min Loss 0.0005437019281089306\n",
            "Iteration: 220/5520  Current Loss: 0.0005924662109464407 and previously recorded Min Loss 0.0005437019281089306\n",
            "Iteration: 221/5530  Current Loss: 0.0005460649263113737 and previously recorded Min Loss 0.0005437019281089306\n",
            "Iteration: 221/5540  Current Loss: 0.0005922233103774488 and previously recorded Min Loss 0.0005437019281089306\n",
            "Iteration: 221/5550  Current Loss: 0.0005729948752559721 and previously recorded Min Loss 0.0005437019281089306\n",
            "Iteration: 222/5560  Current Loss: 0.0006230357685126364 and previously recorded Min Loss 0.0005437019281089306\n",
            "Iteration: 222/5570  Current Loss: 0.0007601769175380468 and previously recorded Min Loss 0.0005437019281089306\n",
            "Iteration: 223/5580  Current Loss: 0.0006392687209881842 and previously recorded Min Loss 0.0005437019281089306\n",
            "Iteration: 223/5590  Current Loss: 0.0006660054787062109 and previously recorded Min Loss 0.0005437019281089306\n",
            "Iteration: 223/5600  Current Loss: 0.000572366698179394 and previously recorded Min Loss 0.0005437019281089306\n",
            "Iteration: 224/5610  Current Loss: 0.0005676935543306172 and previously recorded Min Loss 0.0005437019281089306\n",
            "Iteration: 224/5620  Current Loss: 0.0005657257279381156 and previously recorded Min Loss 0.0005437019281089306\n",
            "Iteration: 225/5630  Current Loss: 0.0005244603962637484 and previously recorded Min Loss 0.0005437019281089306\n",
            " Model saved\n",
            "Iteration: 225/5640  Current Loss: 0.0005828310386277735 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 225/5650  Current Loss: 0.0005824773688800633 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 226/5660  Current Loss: 0.0005413254257291555 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 226/5670  Current Loss: 0.00058237265329808 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 227/5680  Current Loss: 0.0005832840106450021 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 227/5690  Current Loss: 0.0005715152365155518 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 227/5700  Current Loss: 0.0005769930430687964 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 228/5710  Current Loss: 0.0005663917982019484 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 228/5720  Current Loss: 0.0006082304171286523 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 229/5730  Current Loss: 0.0006944823544472456 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 229/5740  Current Loss: 0.0005945637240074575 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 229/5750  Current Loss: 0.0007503947708755732 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 230/5760  Current Loss: 0.0006101393373683095 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 230/5770  Current Loss: 0.0006062848842702806 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 231/5780  Current Loss: 0.0008018686203286052 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 231/5790  Current Loss: 0.0006800906267017126 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 231/5800  Current Loss: 0.0007575697381980717 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 232/5810  Current Loss: 0.000674321549013257 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 232/5820  Current Loss: 0.001441706670448184 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 233/5830  Current Loss: 0.0005462328554131091 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 233/5840  Current Loss: 0.0006960401078686118 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 233/5850  Current Loss: 0.0006399502162821591 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 234/5860  Current Loss: 0.0006171068525873125 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 234/5870  Current Loss: 0.0006646246765740216 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 235/5880  Current Loss: 0.0005469328025355935 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 235/5890  Current Loss: 0.0005795383476652205 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 235/5900  Current Loss: 0.0009103853954002261 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 236/5910  Current Loss: 0.0007682016585022211 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 236/5920  Current Loss: 0.0007772823446430266 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 237/5930  Current Loss: 0.0012288974830880761 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 237/5940  Current Loss: 0.0006618984043598175 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 237/5950  Current Loss: 0.0009681924129836261 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 238/5960  Current Loss: 0.0006142945494502783 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 238/5970  Current Loss: 0.0007262753788381815 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 239/5980  Current Loss: 0.0009793867357075214 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 239/5990  Current Loss: 0.0005895909271202981 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 239/6000  Current Loss: 0.0006261146045289934 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 240/6010  Current Loss: 0.0011586203472688794 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 240/6020  Current Loss: 0.001119243330322206 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 241/6030  Current Loss: 0.0006482212338596582 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 241/6040  Current Loss: 0.0006077558500692248 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 241/6050  Current Loss: 0.0005761486245319247 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 242/6060  Current Loss: 0.000574793666601181 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 242/6070  Current Loss: 0.0006579855689778924 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 243/6080  Current Loss: 0.0007625271682627499 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 243/6090  Current Loss: 0.0015379011165350676 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 243/6100  Current Loss: 0.0013327811611816287 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 244/6110  Current Loss: 0.0009838291443884373 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 244/6120  Current Loss: 0.0008379591163247824 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 245/6130  Current Loss: 0.0011610713554546237 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 245/6140  Current Loss: 0.0008978564874269068 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 245/6150  Current Loss: 0.0005875694332644343 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 246/6160  Current Loss: 0.0007780315936543047 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 246/6170  Current Loss: 0.0008702341001480818 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 247/6180  Current Loss: 0.0006644775858148932 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 247/6190  Current Loss: 0.000892932468559593 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 247/6200  Current Loss: 0.0011152175720781088 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 248/6210  Current Loss: 0.0006108213565312326 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 248/6220  Current Loss: 0.000738773203920573 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 249/6230  Current Loss: 0.0006360452389344573 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 249/6240  Current Loss: 0.0007588578155264258 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 249/6250  Current Loss: 0.0009655284229665995 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 250/6260  Current Loss: 0.0005870899767614901 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 250/6270  Current Loss: 0.000660038844216615 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 251/6280  Current Loss: 0.0005769801791757345 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 251/6290  Current Loss: 0.0005505908629857004 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 251/6300  Current Loss: 0.0006568782264366746 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 252/6310  Current Loss: 0.0006227112025953829 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 252/6320  Current Loss: 0.0005878010997548699 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 253/6330  Current Loss: 0.0005568853230215609 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 253/6340  Current Loss: 0.0005812039598822594 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 253/6350  Current Loss: 0.0007123370887711644 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 254/6360  Current Loss: 0.0006217631744220853 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 254/6370  Current Loss: 0.0006611656863242388 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 255/6380  Current Loss: 0.0006143257487565279 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 255/6390  Current Loss: 0.0007105627446435392 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 255/6400  Current Loss: 0.0006417623953893781 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 256/6410  Current Loss: 0.0013398423325270414 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 256/6420  Current Loss: 0.0017096922965720296 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 257/6430  Current Loss: 0.0005825903499498963 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 257/6440  Current Loss: 0.0006289702141657472 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 257/6450  Current Loss: 0.0009687016136012971 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 258/6460  Current Loss: 0.0008423669496551156 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 258/6470  Current Loss: 0.0007032767753116786 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 259/6480  Current Loss: 0.0006144248764030635 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 259/6490  Current Loss: 0.0005856697680428624 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 259/6500  Current Loss: 0.0005575729883275926 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 260/6510  Current Loss: 0.0005612507811747491 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 260/6520  Current Loss: 0.0005832459428347647 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 261/6530  Current Loss: 0.000554345257114619 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 261/6540  Current Loss: 0.0005389382131397724 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 261/6550  Current Loss: 0.0006561451009474695 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 262/6560  Current Loss: 0.0005306650418788195 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 262/6570  Current Loss: 0.0005854105693288147 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 263/6580  Current Loss: 0.0006046536145731807 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 263/6590  Current Loss: 0.0006292998441495001 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 263/6600  Current Loss: 0.00056585663696751 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 264/6610  Current Loss: 0.000559966079890728 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 264/6620  Current Loss: 0.0006366895977407694 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 265/6630  Current Loss: 0.000589398667216301 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 265/6640  Current Loss: 0.000565501453820616 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 265/6650  Current Loss: 0.0006115507567301393 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 266/6660  Current Loss: 0.0005979628767818213 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 266/6670  Current Loss: 0.0005639319424517453 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 267/6680  Current Loss: 0.000548057199921459 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 267/6690  Current Loss: 0.0005369344144128263 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 267/6700  Current Loss: 0.0005417555803433061 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 268/6710  Current Loss: 0.0005946117453277111 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 268/6720  Current Loss: 0.0005492269992828369 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 269/6730  Current Loss: 0.0005430240998975933 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 269/6740  Current Loss: 0.0005290790577419102 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 269/6750  Current Loss: 0.0007372616091743112 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 270/6760  Current Loss: 0.0006340835243463516 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 270/6770  Current Loss: 0.000654836418107152 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 271/6780  Current Loss: 0.0005682301125489175 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 271/6790  Current Loss: 0.0005919627728872001 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 271/6800  Current Loss: 0.0007859492325223982 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 272/6810  Current Loss: 0.0006302041583694518 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 272/6820  Current Loss: 0.0006869087228551507 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 273/6830  Current Loss: 0.0005661239265464246 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 273/6840  Current Loss: 0.0006106373621150851 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 273/6850  Current Loss: 0.0005649813101626933 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 274/6860  Current Loss: 0.0005802458617836237 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 274/6870  Current Loss: 0.0005579872522503138 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 275/6880  Current Loss: 0.000573989818803966 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 275/6890  Current Loss: 0.0020401382353156805 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 275/6900  Current Loss: 0.0005733614088967443 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 276/6910  Current Loss: 0.0006759222014807165 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 276/6920  Current Loss: 0.0005785798421129584 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 277/6930  Current Loss: 0.000560712069272995 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 277/6940  Current Loss: 0.0005735854501836002 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 277/6950  Current Loss: 0.0006577336462214589 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 278/6960  Current Loss: 0.0006504149059765041 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 278/6970  Current Loss: 0.0007623893325217068 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 279/6980  Current Loss: 0.0006160425837151706 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 279/6990  Current Loss: 0.000567710492759943 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 279/7000  Current Loss: 0.0006504195043817163 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 280/7010  Current Loss: 0.00056884263176471 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 280/7020  Current Loss: 0.000577243510633707 and previously recorded Min Loss 0.0005244603962637484\n",
            "Iteration: 281/7030  Current Loss: 0.0005215116543695331 and previously recorded Min Loss 0.0005244603962637484\n",
            " Model saved\n",
            "Iteration: 281/7040  Current Loss: 0.0006021335138939321 and previously recorded Min Loss 0.0005215116543695331\n",
            "Iteration: 281/7050  Current Loss: 0.0006167438114061952 and previously recorded Min Loss 0.0005215116543695331\n",
            "Iteration: 282/7060  Current Loss: 0.0005674448912031949 and previously recorded Min Loss 0.0005215116543695331\n",
            "Iteration: 282/7070  Current Loss: 0.0005266420193947852 and previously recorded Min Loss 0.0005215116543695331\n",
            "Iteration: 283/7080  Current Loss: 0.000660066376440227 and previously recorded Min Loss 0.0005215116543695331\n",
            "Iteration: 283/7090  Current Loss: 0.0005697553278878331 and previously recorded Min Loss 0.0005215116543695331\n",
            "Iteration: 283/7100  Current Loss: 0.0009866420878097415 and previously recorded Min Loss 0.0005215116543695331\n",
            "Iteration: 284/7110  Current Loss: 0.0006775470683351159 and previously recorded Min Loss 0.0005215116543695331\n",
            "Iteration: 284/7120  Current Loss: 0.0006202067015692592 and previously recorded Min Loss 0.0005215116543695331\n",
            "Iteration: 285/7130  Current Loss: 0.0006250239675864577 and previously recorded Min Loss 0.0005215116543695331\n",
            "Iteration: 285/7140  Current Loss: 0.0006121268961578608 and previously recorded Min Loss 0.0005215116543695331\n",
            "Iteration: 285/7150  Current Loss: 0.0005669483216479421 and previously recorded Min Loss 0.0005215116543695331\n",
            "Iteration: 286/7160  Current Loss: 0.0007584614213556051 and previously recorded Min Loss 0.0005215116543695331\n",
            "Iteration: 286/7170  Current Loss: 0.0008970365743152797 and previously recorded Min Loss 0.0005215116543695331\n",
            "Iteration: 287/7180  Current Loss: 0.0006114636780694127 and previously recorded Min Loss 0.0005215116543695331\n",
            "Iteration: 287/7190  Current Loss: 0.0006444153841584921 and previously recorded Min Loss 0.0005215116543695331\n",
            "Iteration: 287/7200  Current Loss: 0.0007725147879682481 and previously recorded Min Loss 0.0005215116543695331\n",
            "Iteration: 288/7210  Current Loss: 0.0005467098671942949 and previously recorded Min Loss 0.0005215116543695331\n",
            "Iteration: 288/7220  Current Loss: 0.0006619416526518762 and previously recorded Min Loss 0.0005215116543695331\n",
            "Iteration: 289/7230  Current Loss: 0.0006246438715606928 and previously recorded Min Loss 0.0005215116543695331\n",
            "Iteration: 289/7240  Current Loss: 0.0006816815002821386 and previously recorded Min Loss 0.0005215116543695331\n",
            "Iteration: 289/7250  Current Loss: 0.0006130029796622694 and previously recorded Min Loss 0.0005215116543695331\n",
            "Iteration: 290/7260  Current Loss: 0.0007339882431551814 and previously recorded Min Loss 0.0005215116543695331\n",
            "Iteration: 290/7270  Current Loss: 0.0006136285956017673 and previously recorded Min Loss 0.0005215116543695331\n",
            "Iteration: 291/7280  Current Loss: 0.000622287392616272 and previously recorded Min Loss 0.0005215116543695331\n",
            "Iteration: 291/7290  Current Loss: 0.0006155830342322588 and previously recorded Min Loss 0.0005215116543695331\n",
            "Iteration: 291/7300  Current Loss: 0.00061597372405231 and previously recorded Min Loss 0.0005215116543695331\n",
            "Iteration: 292/7310  Current Loss: 0.0006190486601553857 and previously recorded Min Loss 0.0005215116543695331\n",
            "Iteration: 292/7320  Current Loss: 0.0009418220724910498 and previously recorded Min Loss 0.0005215116543695331\n",
            "Iteration: 293/7330  Current Loss: 0.0005857421783730388 and previously recorded Min Loss 0.0005215116543695331\n",
            "Iteration: 293/7340  Current Loss: 0.0011287392117083073 and previously recorded Min Loss 0.0005215116543695331\n",
            "Iteration: 293/7350  Current Loss: 0.0006177545874379575 and previously recorded Min Loss 0.0005215116543695331\n",
            "Iteration: 294/7360  Current Loss: 0.0005872631445527077 and previously recorded Min Loss 0.0005215116543695331\n",
            "Iteration: 294/7370  Current Loss: 0.0006232634768821299 and previously recorded Min Loss 0.0005215116543695331\n",
            "Iteration: 295/7380  Current Loss: 0.0005920849507674575 and previously recorded Min Loss 0.0005215116543695331\n",
            "Iteration: 295/7390  Current Loss: 0.000764508789870888 and previously recorded Min Loss 0.0005215116543695331\n",
            "Iteration: 295/7400  Current Loss: 0.0005948155885562301 and previously recorded Min Loss 0.0005215116543695331\n",
            "Iteration: 296/7410  Current Loss: 0.0005498903919942677 and previously recorded Min Loss 0.0005215116543695331\n",
            "Iteration: 296/7420  Current Loss: 0.0007078383932821453 and previously recorded Min Loss 0.0005215116543695331\n",
            "Iteration: 297/7430  Current Loss: 0.0005553691298700869 and previously recorded Min Loss 0.0005215116543695331\n",
            "Iteration: 297/7440  Current Loss: 0.0005524867447093129 and previously recorded Min Loss 0.0005215116543695331\n",
            "Iteration: 297/7450  Current Loss: 0.0005724354414269328 and previously recorded Min Loss 0.0005215116543695331\n",
            "Iteration: 298/7460  Current Loss: 0.0005601666052825749 and previously recorded Min Loss 0.0005215116543695331\n",
            "Iteration: 298/7470  Current Loss: 0.0005698812892660499 and previously recorded Min Loss 0.0005215116543695331\n",
            "Iteration: 299/7480  Current Loss: 0.0005865661660209298 and previously recorded Min Loss 0.0005215116543695331\n",
            "Iteration: 299/7490  Current Loss: 0.0005672492552548647 and previously recorded Min Loss 0.0005215116543695331\n",
            "Iteration: 299/7500  Current Loss: 0.0005985858733765781 and previously recorded Min Loss 0.0005215116543695331\n",
            "Minimum loss 0.0005215116543695331\n",
            "Total Learning time 592.928531, Learning time without validation 423.7735046720075, Average Learning time 1.5073682676633668\n",
            "Model took 641.01289762 seconds\n"
          ]
        }
      ],
      "source": [
        "# Training Model\n",
        "print(config.X_train_path_single)\n",
        "X_train_org = pickle.load(open(config.X_train_path_single, 'rb'))\n",
        "X_train, X_valid = run_model.train_test_split(X_train_org, test_size=0.2, random_state=42, shuffle=True)  # Split the set\n",
        "print(len(X_train))\n",
        "# running model\n",
        "tic = time.perf_counter()\n",
        "run_model.main(np.array(X_train), np.array(X_valid), config.fp)\n",
        "toc = time.perf_counter()\n",
        "print(\"Model took {} seconds\".format((toc - tic)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwHR-jmwREtZ"
      },
      "source": [
        "## Model Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OjAenAw0REtZ",
        "outputId": "72729ea3-50be-47f1-d1a3-9275d8f5eb09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running for v_0.78_\n",
            "Model loaded successfully\n",
            "Running tensormaker\n",
            "Test Input Data Transformed\n",
            "Test Target Data Transformed\n",
            "Starting files set v_0.78_\n",
            "X file data/model_purpose/PT_S38417/v_0.78_test.pk\n",
            " Model saved\n",
            "Model from v_0.78_, Test data v_0.78_, Loss: 0.00039662918425165117 \n"
          ]
        }
      ],
      "source": [
        "# Testing Model\n",
        "combined_setup_list = [config.fp]\n",
        "exp_name_list = [config.fp]\n",
        "for i in range(len(exp_name_list)):\n",
        "    print('Running for {}'.format(exp_name_list[i]))\n",
        "    chkpoint_filename = config.model_chkpnt_path + combined_setup_list[i] + 'model_min_loss.ckpt'\n",
        "    std_scalar_filename = config.model_chkpnt_path + combined_setup_list[i] + 'std_scaler_train.pk'\n",
        "    std_y_filename = config.model_chkpnt_path + combined_setup_list[i] + 'sc_y_train.pk'\n",
        "    training_time_filename = config.model_chkpnt_path + combined_setup_list[i] + 'training_time.pk'\n",
        "    exp_name=exp_name_list[i]\n",
        "    predict_from_chkpnt.predict_from_chkpnt(chkpoint_filename, std_scalar_filename, std_y_filename, exp_name, training_time_filename)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mo8tXJGREta"
      },
      "source": [
        "## Performance Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CexN1eEhREta",
        "outputId": "e74b3c9b-c979-4c7b-db44-b2e5fc06d876"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Std error 10.302479950052989, mean error -0.7800114884800924, mse 0.0001067495110434048, errors_percentage 0.9937716081289266\n"
          ]
        }
      ],
      "source": [
        "# Metrics\n",
        "# reading processed data\n",
        "labels = pickle.load(open(config.label_file, 'rb')).flatten()\n",
        "outputs = pickle.load(open(config.output_file, 'rb')).flatten()\n",
        "errors = labels - outputs\n",
        "sigma = np.std(errors) * 1000  # ns to ps\n",
        "mu = np.ndarray.mean(errors) * 1000  # ns to ps\n",
        "errors_percentage = (np.std(errors) / max(labels)) * 100\n",
        "mse = np.square(np.subtract(labels,outputs)).mean()\n",
        "print('Std error {}, mean error {}, mse {}, errors_percentage {}'.format(sigma, mu, mse, errors_percentage))\n",
        "filename = config.model_chkpnt_path+config.fp\n",
        "utility.save_as_csv(labels, outputs, errors, filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "otazZDzwREta"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:CAD]",
      "language": "python",
      "name": "conda-env-CAD-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}